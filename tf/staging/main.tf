

resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"
  tags = {
    project = "ram"
  }
}

resource "aws_s3_bucket" "input_data" {
  bucket = "research-ram-source-data"
}

resource "aws_s3_bucket" "research_data" {
  bucket = "research-ram-data"
}

resource "aws_athena_database" "research" {
  name   = "research"
  bucket = aws_s3_bucket.research_data.bucket
}

resource "aws_athena_workgroup" "research_ram" {
  name = "research-ram"

  configuration {
    enforce_workgroup_configuration    = true
    publish_cloudwatch_metrics_enabled = true

    result_configuration {
      output_location = "s3://${aws_s3_bucket.research_data.bucket}/output/"

      encryption_configuration {
        encryption_option = "SSE_S3"
      }
    }
  }
}

resource "aws_athena_named_query" "test_iris" {
  name      = "test-usernames"
  workgroup = aws_athena_workgroup.research_ram.id
  database  = aws_athena_database.research.id
  query     = "SELECT * FROM \"${aws_glue_catalog_table.avro_test.name}\" limit 10;"
}

resource "aws_glue_registry" "research" {
  registry_name = "research-ram"
}

resource "aws_glue_schema" "avro_test" {
  schema_name       = "avro-test"
  registry_arn      = aws_glue_registry.research.arn
  data_format       = "AVRO"
  compatibility     = "NONE"
  schema_definition = "{\"type\":\"record\",\"name\":\"kylosample\",\"doc\":\"Schema generated by Kite\",\"fields\":[{\"name\":\"registration_dttm\",\"type\":\"string\",\"doc\":\"Type inferred from '2016-02-03T16:07:46Z'\"},{\"name\":\"id\",\"type\":\"long\",\"doc\":\"Type inferred from '1'\"},{\"name\":\"first_name\",\"type\":\"string\",\"doc\":\"Type inferred from 'Ernest'\"},{\"name\":\"last_name\",\"type\":\"string\",\"doc\":\"Type inferred from 'Fuller'\"},{\"name\":\"email\",\"type\":\"string\",\"doc\":\"Type inferred from 'efuller0@examiner.com'\"},{\"name\":\"gender\",\"type\":\"string\",\"doc\":\"Type inferred from 'Male'\"},{\"name\":\"ip_address\",\"type\":\"string\",\"doc\":\"Type inferred from '106.72.28.74'\"},{\"name\":\"cc\",\"type\":[\"null\",\"long\"],\"doc\":\"Type inferred from '5610608195667267'\",\"default\":null},{\"name\":\"country\",\"type\":\"string\",\"doc\":\"Type inferred from 'Israel'\"},{\"name\":\"birthdate\",\"type\":\"string\",\"doc\":\"Type inferred from '1/16/1998'\"},{\"name\":\"salary\",\"type\":[\"null\",\"double\"],\"doc\":\"Type inferred from '140639.36'\",\"default\":null},{\"name\":\"title\",\"type\":\"string\",\"doc\":\"Type inferred from 'Developer II'\"},{\"name\":\"comments\",\"type\":\"string\",\"doc\":\"Type inferred from '.. .. .. .. .. .. .. .. '\"}]}"
}

resource "aws_glue_catalog_table" "avro_test" {
  database_name = aws_glue_catalog_database.research.name
  name          = "research-avro-test"

  parameters = {
    EXTERNAL       = "TRUE"
    classification = "avro"
    "avro.compression" = "SNAPPY"
  }

  storage_descriptor {
    schema_reference {
      schema_id {
        schema_name   = aws_glue_schema.avro_test.schema_name
        registry_name = aws_glue_schema.avro_test.registry_name
      }
      schema_version_number = aws_glue_schema.avro_test.latest_schema_version
    }

    location      = "s3://${aws_s3_bucket.input_data.bucket}/userdata3.avro"
    input_format  = "org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat"
    output_format = "org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat"

    ser_de_info {
      serialization_library = "org.apache.hadoop.hive.serde2.avro.AvroSerDe"
      parameters = {
        "serialization.format" = 1
      }
    }

  }
}

resource "aws_glue_catalog_database" "research" {
  name = "research-ram"
}

# darn, avro classifier cannot be declared with terraform...
# is it because there is nothing to configure?
#
# resource "aws_glue_classifier" "avro_classifier" {
#   name = "avro-classifier"
# }

# It would be awesome to use aws timestream, a dedicated time series database
# however, data can only be inserted within the "active" window:
# from (now - retention_period) until (now + injestion period)
# also, data should be inserted in chronological order
# this, perhaps, disqualifies timestream from current task, processing historical data
